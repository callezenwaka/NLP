{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Exercise.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/callezenwaka/NLP/blob/master/NLP_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZ-5DVydhl1",
        "colab_type": "text"
      },
      "source": [
        "# **Data Cleaning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CidX5ps2duXY",
        "colab_type": "text"
      },
      "source": [
        "# Getting The Data¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YSgxRMhPGk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "# Web scraping, pickle imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAykQu-nPYJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scrapes transcript data from scrapsfromtheloft.com\n",
        "def url_to_transcript(url):\n",
        "    '''Returns transcript data specifically from scrapsfromtheloft.com.'''\n",
        "    page = requests.get(url).text\n",
        "    soup = BeautifulSoup(page, \"lxml\")\n",
        "    text = [p.text for p in soup.find(class_=\"post-content\").find_all('p')]\n",
        "    print(url)\n",
        "    return text\n",
        "\n",
        "# URLs of transcripts in scope\n",
        "urls = ['http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/07/bo-burnham-2013-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/10/21/hasan-minhaj-homecoming-king-2017-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/']\n",
        "\n",
        "# Comedian names\n",
        "comedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T0QmhnhPYL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Actually request transcripts (takes a few minutes to run)\n",
        "transcripts = [url_to_transcript(u) for u in urls]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaZu4EIKPYOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Pickle files for later use\n",
        "\n",
        "# Make a new directory to hold the text files\n",
        "!mkdir transcripts\n",
        "\n",
        "for i, c in enumerate(comedians):\n",
        "    with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
        "        pickle.dump(transcripts[i], file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1SYpGbLsYua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load pickled files\n",
        "data = {}\n",
        "for i, c in enumerate(comedians):\n",
        "    with open(\"transcripts/\" + c + \".txt\", \"rb\") as file:\n",
        "        data[c] = pickle.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXbo9BdDsYwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Double check to make sure data has been loaded properly\n",
        "data.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqmCC9XVsYyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# More checks\n",
        "data['louis'][:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9peXu9QoweZt",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning The Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npe6Dyjuwfy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at our data again\n",
        "next(iter(data.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-SnWBXvw-2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
        "next(iter(data.values()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkQ7OOwJw-5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are going to change this to key: comedian, value: string format\n",
        "def combine_text(list_of_text):\n",
        "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
        "    combined_text = ' '.join(list_of_text)\n",
        "    return combined_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52V5G2iKw-8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine it!\n",
        "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9NIe8LAw_AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth',150)\n",
        "\n",
        "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
        "data_df.columns = ['transcript']\n",
        "data_df = data_df.sort_index()\n",
        "data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtHNlH0w_C3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Let's take a look at the transcript for Ali Wong\n",
        "data_df.transcript.loc['ali']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCSMHSTdzHuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply a first round of text cleaning techniques\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text_round1(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "round1 = lambda x: clean_text_round1(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJgRFm7gzHxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at the updated text\n",
        "data_clean = pd.DataFrame(data_df.transcript.apply(round1))\n",
        "data_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDlqsXCY0AN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply a second round of cleaning\n",
        "def clean_text_round2(text):\n",
        "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "\n",
        "round2 = lambda x: clean_text_round2(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IQrgggA0AdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at the updated text\n",
        "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
        "data_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anJfdS2BWXq5",
        "colab_type": "text"
      },
      "source": [
        "# Organizing The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZPlcIbqWdj4",
        "colab_type": "text"
      },
      "source": [
        "Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo98DoWP0Af0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at our dataframe\n",
        "data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuxgzw7fWifE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's add the comedians' full names as well\n",
        "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',\n",
        "              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
        "\n",
        "data_df['full_name'] = full_names\n",
        "data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKQ_rEuLWiiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's pickle it for later use\n",
        "data_df.to_pickle(\"corpus.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47BttMDuYvd8",
        "colab_type": "text"
      },
      "source": [
        "Document-Term Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2csoedUWikd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "data_cv = cv.fit_transform(data_clean.transcript)\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_dtm.index = data_clean.index\n",
        "data_dtm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrOrVDGzWin5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's pickle it for later use\n",
        "data_dtm.to_pickle(\"dtm.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd1B0E5azH0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
        "data_clean.to_pickle('data_clean.pkl')\n",
        "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSMxR_VsdJEY",
        "colab_type": "text"
      },
      "source": [
        "# **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7ngROIldNeZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Most Common Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETo1JhXid5_q",
        "colab_type": "text"
      },
      "source": [
        "Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZqWcY4NzH43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the document-term matrix\n",
        "data = pd.read_pickle('dtm.pkl')\n",
        "data = data.transpose()\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8gCxVF9d7ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the top 30 words said by each comedian\n",
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "    top = data[c].sort_values(ascending=False).head(30)\n",
        "    top_dict[c]= list(zip(top.index, top.values))\n",
        "\n",
        "top_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ej6VqYc2d7hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the top 15 words said by each comedian\n",
        "for comedian, top_words in top_dict.items():\n",
        "    print(comedian)\n",
        "    print(', '.join([word for word, count in top_words[0:14]]))\n",
        "    print('---')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OfTj_hFd7nY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at the most common top words --> add them to the stop word list\n",
        "from collections import Counter\n",
        "\n",
        "# Let's first pull out the top 30 words for each comedian\n",
        "words = []\n",
        "for comedian in data.columns:\n",
        "    top = [word for (word, count) in top_dict[comedian]]\n",
        "    for t in top:\n",
        "        words.append(t)\n",
        "        \n",
        "words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfrO_hRpd7kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's aggregate this list and identify the most common words along with how many routines they occur in\n",
        "Counter(words).most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_yZ2vCDsY01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If more than half of the comedians have it as a top word, exclude it from the list\n",
        "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
        "add_stop_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIYw2vjzhrjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's update our document-term matrix with the new list of stop words\n",
        "from sklearn.feature_extraction import text \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Read in cleaned data\n",
        "data_clean = pd.read_pickle('data_clean.pkl')\n",
        "\n",
        "# Add new stop words\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "\n",
        "# Recreate document-term matrix\n",
        "cv = CountVectorizer(stop_words=stop_words)\n",
        "data_cv = cv.fit_transform(data_clean.transcript)\n",
        "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_stop.index = data_clean.index\n",
        "\n",
        "# Pickle it for later use\n",
        "import pickle\n",
        "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
        "data_stop.to_pickle(\"dtm_stop.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KPzqNf0hro_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's make some word clouds!\n",
        "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_CFTgTihrrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reset the output dimensions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [16, 6]\n",
        "\n",
        "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',\n",
        "              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
        "\n",
        "# Create subplots for each comedian\n",
        "for index, comedian in enumerate(data.columns):\n",
        "    wc.generate(data_clean.transcript[comedian])\n",
        "    \n",
        "    plt.subplot(3, 4, index+1)\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(full_names[index])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qbb6RPdfidpq",
        "colab_type": "text"
      },
      "source": [
        "# Number of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BblvKaosifYU",
        "colab_type": "text"
      },
      "source": [
        "Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtRocHmhhrnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the number of unique words that each comedian uses\n",
        "\n",
        "# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n",
        "unique_list = []\n",
        "for comedian in data.columns:\n",
        "    uniques = data[comedian].to_numpy().nonzero()[0].size\n",
        "    unique_list.append(uniques)\n",
        "\n",
        "# Create a new dataframe that contains this unique word count\n",
        "data_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])\n",
        "data_unique_sort = data_words.sort_values(by='unique_words')\n",
        "data_unique_sort"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZlhOCXlihcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the total number of words that a comedian uses\n",
        "total_list = []\n",
        "for comedian in data.columns:\n",
        "    totals = sum(data[comedian])\n",
        "    total_list.append(totals)\n",
        "    \n",
        "# Comedy special run times from IMDB, in minutes\n",
        "run_times = [60, 59, 80, 60, 67, 73, 77, 63, 62, 58, 76, 79]\n",
        "\n",
        "# Let's add some columns to our dataframe\n",
        "data_words['total_words'] = total_list\n",
        "data_words['run_times'] = run_times\n",
        "data_words['words_per_minute'] = data_words['total_words'] / data_words['run_times']\n",
        "\n",
        "# Sort the dataframe by words per minute to see who talks the slowest and fastest\n",
        "data_wpm_sort = data_words.sort_values(by='words_per_minute')\n",
        "data_wpm_sort"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLuGyLW_ihfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's plot our findings\n",
        "import numpy as np\n",
        "\n",
        "y_pos = np.arange(len(data_words))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(y_pos, data_unique_sort.unique_words, align='center')\n",
        "plt.yticks(y_pos, data_unique_sort.comedian)\n",
        "plt.title('Number of Unique Words', fontsize=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.barh(y_pos, data_wpm_sort.words_per_minute, align='center')\n",
        "plt.yticks(y_pos, data_wpm_sort.comedian)\n",
        "plt.title('Number of Words Per Minute', fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZW8AJNijrWN",
        "colab_type": "text"
      },
      "source": [
        "# Amount of Profanity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLCF-kpXjsjG",
        "colab_type": "text"
      },
      "source": [
        "Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m00F1cIbjuLn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Earlier I said we'd revisit profanity. Let's take a look at the most common words again.\n",
        "Counter(words).most_common()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYQwjhWHjuOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's isolate just these bad words\n",
        "data_bad_words = data.transpose()[['fucking', 'fuck', 'shit']]\n",
        "data_profanity = pd.concat([data_bad_words.fucking + data_bad_words.fuck, data_bad_words.shit], axis=1)\n",
        "data_profanity.columns = ['f_word', 's_word']\n",
        "data_profanity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_IDdLFwjuRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a scatter plot of our findings\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "\n",
        "for i, comedian in enumerate(data_profanity.index):\n",
        "    x = data_profanity.f_word.loc[comedian]\n",
        "    y = data_profanity.s_word.loc[comedian]\n",
        "    plt.scatter(x, y, color='blue')\n",
        "    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)\n",
        "    plt.xlim(-5, 155) \n",
        "    \n",
        "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
        "plt.xlabel('Number of F Bombs', fontsize=15)\n",
        "plt.ylabel('Number of S Words', fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOAFr5Lfm_OE",
        "colab_type": "text"
      },
      "source": [
        "# **Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUhws4aVnCth",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment of Routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCos3L_gjuT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll start by reading in the corpus, which preserves word order\n",
        "data = pd.read_pickle('corpus.pkl')\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opCOU4khnN2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create quick lambda functions to find the polarity and subjectivity of each routine\n",
        "from textblob import TextBlob\n",
        "\n",
        "pol = lambda x: TextBlob(x).sentiment.polarity\n",
        "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "data['polarity'] = data['transcript'].apply(pol)\n",
        "data['subjectivity'] = data['transcript'].apply(sub)\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUGdqSomnN6e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "\n",
        "for index, comedian in enumerate(data.index):\n",
        "    x = data.polarity.loc[comedian]\n",
        "    y = data.subjectivity.loc[comedian]\n",
        "    plt.scatter(x, y, color='blue')\n",
        "    plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)\n",
        "    plt.xlim(-.01, .12) \n",
        "    \n",
        "plt.title('Sentiment Analysis', fontsize=20)\n",
        "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
        "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KENMTjRHnnhu",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment of Routine Over Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFl2zgzinN94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split each routine into 10 parts\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def split_text(text, n=10):\n",
        "    '''Takes in a string of text and splits into n equal parts, with a default of 10 equal parts.'''\n",
        "\n",
        "    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n",
        "    length = len(text)\n",
        "    size = math.floor(length / n)\n",
        "    start = np.arange(0, length, size)\n",
        "    \n",
        "    # Pull out equally sized pieces of text and put it into a list\n",
        "    split_list = []\n",
        "    for piece in range(n):\n",
        "        split_list.append(text[start[piece]:start[piece]+size])\n",
        "    return split_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DkxEEzjnOBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at our data again\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rawhG8aunOED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a list to hold all of the pieces of text\n",
        "list_pieces = []\n",
        "for t in data.transcript:\n",
        "    split = split_text(t)\n",
        "    list_pieces.append(split)\n",
        "    \n",
        "list_pieces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFOf6Miaihih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The list has 10 elements, one for each transcript\n",
        "len(list_pieces)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCHbDExXn-1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each transcript has been split into 10 pieces of text\n",
        "len(list_pieces[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83H4vBTwn-4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate the polarity for each piece of text\n",
        "\n",
        "polarity_transcript = []\n",
        "for lp in list_pieces:\n",
        "    polarity_piece = []\n",
        "    for p in lp:\n",
        "        polarity_piece.append(TextBlob(p).sentiment.polarity)\n",
        "    polarity_transcript.append(polarity_piece)\n",
        "    \n",
        "polarity_transcript"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdX7SxVan-7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show the plot for one comedian\n",
        "plt.plot(polarity_transcript[0])\n",
        "plt.title(data['full_name'].index[0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lOFoXkKn--L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Show the plot for all comedians\n",
        "plt.rcParams['figure.figsize'] = [16, 12]\n",
        "\n",
        "for index, comedian in enumerate(data.index):    \n",
        "    plt.subplot(3, 4, index+1)\n",
        "    plt.plot(polarity_transcript[index])\n",
        "    plt.plot(np.arange(0,10), np.zeros(10))\n",
        "    plt.title(data['full_name'][index])\n",
        "    plt.ylim(ymin=-.2, ymax=.3)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qjvgrqy_DlYq",
        "colab_type": "text"
      },
      "source": [
        "# **Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-Y02SZoDoiM",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling - Attempt #1 (All Text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuead90wn_A2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Let's read in our document-term matrix\n",
        "data = pd.read_pickle('dtm_stop.pkl')\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzkDqwh7DwAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the necessary modules for LDA with gensim\n",
        "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
        "from gensim import matutils, models\n",
        "import scipy.sparse\n",
        "\n",
        "# import logging\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhO26lPlDwDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One of the required inputs is a term-document matrix\n",
        "tdm = data.transpose()\n",
        "tdm.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grYzKeIJDwH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
        "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
        "corpus = matutils.Sparse2Corpus(sparse_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R-Mn6wPEXCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
        "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
        "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UViikdqEXFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
        "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp_LsVgdEXIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA for num_topics = 3\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0oC6T-wEXLg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# LDA for num_topics = 4\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
        "lda.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pfvRPGuE4fQ",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling - Attempt #2 (Nouns Only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BHJeU5jE6ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a function to pull out nouns from a string of text\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "def nouns(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
        "    is_noun = lambda pos: pos[:2] == 'NN'\n",
        "    tokenized = word_tokenize(text)\n",
        "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
        "    return ' '.join(all_nouns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHPAf85gDwGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the cleaned data, before the CountVectorizer step\n",
        "data_clean = pd.read_pickle('data_clean.pkl')\n",
        "data_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK9B9yhhMlJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download nltk\n",
        "import nltk\n",
        "# nltk.download()\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ABthJQdFA13",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the nouns function to the transcripts to filter only on nouns\n",
        "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
        "data_nouns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbjfksTZFBAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new document-term matrix using only nouns\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Re-add the additional stop words since we are recreating the document-term matrix\n",
        "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
        "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "\n",
        "# Recreate a document-term matrix with only nouns\n",
        "cvn = CountVectorizer(stop_words=stop_words)\n",
        "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
        "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
        "data_dtmn.index = data_nouns.index\n",
        "data_dtmn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rma-Ly6BFBFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the gensim corpus\n",
        "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX5aux_5FTdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's start with 2 topics\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFw8kIqPFTgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try topics = 3\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIwJAhOoFTla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try 4 topics\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aaT3wQrSk8y",
        "colab_type": "text"
      },
      "source": [
        "# *Topic Modeling - Attempt #3 (Nouns and Adjectives) *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2Lrh2V7FBIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create a function to pull out nouns from a string of text\n",
        "def nouns_adj(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
        "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
        "    tokenized = word_tokenize(text)\n",
        "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
        "    return ' '.join(nouns_adj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQTAnla-FA0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply the nouns function to the transcripts to filter only on nouns and adj\n",
        "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
        "data_nouns_adj"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h-3H9e5QVgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
        "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
        "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
        "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
        "data_dtmna.index = data_nouns_adj.index\n",
        "data_dtmna"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d59cQEh1QVju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the gensim corpus\n",
        "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkQ0GSv_QV0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's start with 2 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78s8qoeZQpN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try 3 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ro-XjkvQpVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's try 4 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5jzTX31RGU5",
        "colab_type": "text"
      },
      "source": [
        "# Identify Topics in Each Document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrOu_51fQpZY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our final LDA model (for now)\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
        "ldana.print_topics()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReQ_E5vfQpTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's take a look at which topics each transcript contains\n",
        "corpus_transformed = ldana[corpusna]\n",
        "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpQi3JmbcKfh",
        "colab_type": "text"
      },
      "source": [
        "# **Text Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_f2rh3zcbtr",
        "colab_type": "text"
      },
      "source": [
        "# Select Text to Imitate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP3DM0HXQpRs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in the corpus, including punctuation!\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_pickle('corpus.pkl')\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3B80e8dQVdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract only Ali Wong's text\n",
        "ali_text = data.transcript.loc['ali']\n",
        "ali_text[:200]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrNN5mJOT2zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build a Markov Chain Function\n",
        "from collections import defaultdict\n",
        "\n",
        "def markov_chain(text):\n",
        "    '''The input is a string of text and the output will be a dictionary with each word as\n",
        "       a key and each value as the list of words that come after the key in the text.'''\n",
        "    \n",
        "    # Tokenize the text by word, though including punctuation\n",
        "    words = text.split(' ')\n",
        "    \n",
        "    # Initialize a default dictionary to hold all of the words and next words\n",
        "    m_dict = defaultdict(list)\n",
        "    \n",
        "    # Create a zipped list of all of the word pairs and put them in word: list of next words format\n",
        "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
        "        m_dict[current_word].append(next_word)\n",
        "\n",
        "    # Convert the default dict back into a dictionary\n",
        "    m_dict = dict(m_dict)\n",
        "    return m_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaSiOP0ST3IX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the dictionary for Ali's routine, take a look at it\n",
        "ali_dict = markov_chain(ali_text)\n",
        "ali_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fZAavhPT3as",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a Text Generator\n",
        "import random\n",
        "\n",
        "def generate_sentence(chain, count=15):\n",
        "    '''Input a dictionary in the format of key = current word, value = list of next words\n",
        "       along with the number of words you would like to see in your generated sentence.'''\n",
        "\n",
        "    # Capitalize the first word\n",
        "    word1 = random.choice(list(chain.keys()))\n",
        "    sentence = word1.capitalize()\n",
        "\n",
        "    # Generate the second word from the value list. Set the new word as the first word. Repeat.\n",
        "    for i in range(count-1):\n",
        "        word2 = random.choice(chain[word1])\n",
        "        word1 = word2\n",
        "        sentence += ' ' + word2\n",
        "\n",
        "    # End it with a period\n",
        "    sentence += '.'\n",
        "    return(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB9gze66T3en",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build sentence by calling generate sentence function\n",
        "generate_sentence(ali_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T-wxyMRT3F6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gvGOAIWT2wp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}